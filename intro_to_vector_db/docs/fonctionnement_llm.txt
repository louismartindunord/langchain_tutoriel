Fonctionnement LLM 
L'intelligence artificielle générative (GenAI), notamment ChatGPT, capte l'attention de tous. Les grands modèles de langage (LLM) basés sur des transformateurs, formés à grande échelle sur une grande quantité de données non étiquetées, démontrent la capacité de se généraliser à de nombreuses tâches différentes. Pour comprendre pourquoi les LLM sont si puissants, nous approfondirons leur fonctionnement dans cet article.


Arbre évolutif LLM

Formellement, un modèle de langage de décodeur uniquement est simplement une distribution conditionnelle p(xi|x1···xi−1)sur les prochains jetons xidans des contextes donnés x1 · · · xi−1. Une telle formulation est un exemple de processus de Markov, qui a été étudié dans de nombreux cas d'utilisation. Cette configuration simple nous permet également de générer jeton par jeton de manière autorégressive.
Avant notre plongée en profondeur, je dois souligner les limites de cette formulation pour atteindre l'intelligence artificielle générale ( AGI ). La pensée est un processus non linéaire, mais notre appareil de communication, la bouche, ne peut parler que de manière linéaire. Le langage apparaît donc comme une séquence linéaire de mots. C'est un début raisonnable pour modéliser un langage avec un processus de Markov. Mais je soupçonne que cette formulation peut capturer complètement le processus de réflexion (ou AGI). D’un autre côté, la pensée et le langage sont interdépendants. Un modèle de langage suffisamment fort peut encore démontrer une certaine sorte de capacité de réflexion, comme le montre GPT4. Dans ce qui suit, examinons les innovations scientifiques qui font que les LLM apparaissent intelligemment .
Transformer
Il existe de nombreuses façons de modéliser/représenter la distribution conditionnelle p(xi|x1···xi−1). Dans les LLM, nous tentons d'estimer cette distribution conditionnelle avec une architecture de réseau neuronal appelée Transformer. En fait, les réseaux de neurones, en particulier une variété de réseaux de neurones récurrents (RNN), ont été utilisés dans la modélisation du langage depuis longtemps avant Transformer. Les RNN traitent les jetons de manière séquentielle, en conservant un vecteur d'état qui contient une représentation des données vues avant le jeton actuel. Pour traiter le n-ème jeton, le modèle combine l'état représentant la phrase jusqu'au jeton n-1avec les informations du nouveau jeton pour créer un nouvel état, représentant la phrase jusqu'au jeton.n. Théoriquement, les informations d'un jeton peuvent se propager arbitrairement loin dans la séquence, si à chaque instant l'état continue de coder des informations contextuelles sur le jeton. Malheureusement, le problème du gradient de disparition laisse l'état du modèle à la fin d'une longue phrase sans informations précises et extractibles sur les jetons précédents. La dépendance des calculs de jetons sur les résultats des calculs de jetons précédents rend également difficile la parallélisation des calculs sur le matériel GPU moderne.
Ces problèmes ont été résolus par des mécanismes d'auto-attention dans Transformer . Transformer est une architecture de modèle qui évite la récurrence et s'appuie entièrement sur un mécanisme d'attention pour établir des dépendances globales entre l'entrée et la sortie. La couche d'attention peut accéder à tous les états précédents et les peser selon une mesure de pertinence apprise, fournissant ainsi des informations pertinentes sur les jetons éloignés. Il est important de noter que les Transformers utilisent un mécanisme d'attention sans RNN, traitant tous les jetons simultanément et calculant les pondérations d'attention entre eux dans des couches successives. Étant donné que le mécanisme d'attention utilise uniquement des informations sur les autres jetons des couches inférieures, il peut être calculé pour tous les jetons en parallèle, ce qui entraîne une vitesse d'entraînement améliorée.

Le texte d'entrée est analysé en jetons par un tokeniseur de paires d'octets, et chaque jeton est converti en un vecteur d'intégration. Ensuite, les informations de position du jeton sont ajoutées à l'intégration. Les blocs de construction du transformateur sont des unités d'attention de produit scalaire à l'échelle. Lorsqu'une phrase est transmise dans un modèle de transformateur, des poids d'attention sont calculés simultanément entre chaque jeton. L'unité d'attention produit des intégrations pour chaque jeton dans le contexte qui contiennent des informations sur le jeton lui-même ainsi qu'une combinaison pondérée d'autres jetons pertinents, chacun pondéré par son poids d'attention.
Pour chaque unité d'attention, le modèle de transformateur apprend trois matrices de poids ; la requête pondère WQ, la clé pondère WK et la valeur pondère WV. Pour chaque jeton i , l'intégration du mot d'entrée est multipliée par chacune des trois matrices de poids pour produire un vecteur de requête qi, un vecteur clé ki et un vecteur de valeur vi. Les poids d'attention sont un produit scalaire entre qi et kj, mis à l'échelle par la racine carrée de la dimension des vecteurs clés et normalisés via softmax. La sortie de l'unité d'attention pour le jeton i est la somme pondérée des vecteurs de valeur de tous les jetons, pondérée par l'attention du jeton i à chaque jeton j . Le calcul de l'attention pour tous les jetons peut être exprimé sous la forme d'un grand calcul matriciel :

Un ensemble de matrices (WQ, WK, WV) est appelé tête d'attention, et chaque couche de transformateur possède plusieurs têtes d'attention. Avec plusieurs têtes d'attention, le modèle peut calculer différentes pertinences entre les jetons. Les calculs pour chaque tête d'attention peuvent être effectués en parallèle et les sorties sont concaténées et projetées vers la même dimension d'entrée par une matrice WO.
Dans un codeur, il existe un perceptron multicouche (MLP) entièrement connecté après le mécanisme d’auto-attention. Le bloc MLP traite en outre chaque codage de sortie individuellement. Dans le cadre codeur-décodeur (par exemple pour la traduction), un mécanisme d'attention supplémentaire est inséré entre l'auto-attention et le MLP dans le décodeur pour extraire les informations pertinentes des codages générés par les codeurs. Dans une architecture avec décodeur uniquement, cela n’est pas nécessaire. Quelle que soit l'architecture du codeur-décodeur ou du décodeur uniquement, le décodeur ne doit pas utiliser la sortie actuelle ou future pour prédire une sortie, la séquence de sortie doit donc être partiellement masquée pour empêcher ce flux d'informations inverse, ce qui permet la génération de texte autorégressive. Pour générer jeton par jeton, le dernier décodeur est suivi d'une couche softmax pour produire les probabilités de sortie sur le vocabulaire.

Mise au point supervisée
Le GPT avec décodeur uniquement est essentiellement un algorithme de pré-entraînement non supervisé (ou auto-supervisé) qui maximise la probabilité suivante :

où k est la taille de la fenêtre contextuelle . Bien que l'architecture soit indépendante des tâches, GPT démontre que des gains importants en matière d'inférence en langage naturel, de réponse aux questions, de similarité sémantique et de classification de texte peuvent être réalisés par un pré- entraînement génératif d'un modèle de langage sur un corpus diversifié de texte non étiqueté, suivi d'un apprentissage discriminatif. mise au point sur chaque tâche spécifique.
Après avoir pré-entraîné le modèle avec l'objectif ci-dessus, nous pouvons adapter les paramètres à la tâche cible supervisée. Étant donné un ensemble de données étiqueté C, où chaque instance consiste en une séquence de jetons d'entrée, x1, . . . , xm, avec une étiquette y. Les entrées passent par le modèle pré-entraîné pour obtenir le hlm d'activation du bloc de transformateur final, qui est ensuite introduit dans une couche de sortie linéaire ajoutée avec les paramètres Wy pour prédire y :

En conséquence, nous avons la fonction objectif suivante :

De plus, il est utile d’inclure la modélisation du langage comme objectif auxiliaire car elle améliore la généralisation du modèle supervisé et accélère la convergence. Autrement dit, nous optimisons l’objectif suivant :

La classification du texte peut être directement affinée comme décrit ci-dessus. D'autres tâches, comme la réponse à des questions ou l'implication textuelle, ont des entrées structurées telles que des paires de phrases ordonnées ou des triplets de document, de question et de réponses. Étant donné que le modèle pré-entraîné a été formé sur des séquences de texte contiguës, il nécessite quelques modifications pour s'appliquer à ces tâches.
